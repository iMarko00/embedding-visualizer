import express from "express";
import bodyParser from "body-parser";
import OpenAI from "openai";
import { PCA } from "ml-pca"; // for dimensionality reduction
import { DBSCAN } from "density-clustering"; // for clustering
import { kmeans } from "ml-kmeans"; // fallback clustering
import axios from "axios";

const MARQO_URL = "http://localhost:8882";
const INDEX_NAME = "earlybird_requirements";

const app = express();
const port = 3000;

app.use(express.static("public"));
app.use(bodyParser.json());

console.log("ğŸ”‘ OpenAI API key loaded?", !!process.env.OPENAI_API_KEY);

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Helper function to process clusters and generate names
async function processClusters(clusters, requirements, reduced, client) {
  const clusterMap = new Map();

  // Generate cluster names using OpenAI
  console.log("ğŸ·ï¸ Generating cluster names with OpenAI...");
  const clusterNames = await Promise.all(
    Array.from(clusters.entries()).map(async ([clusterId, indices]) => {
      const clusterRequirements = indices.map(i => requirements[i]);
      console.log(`ğŸ“ Cluster ${clusterId} requirements:`, clusterRequirements);
      
      try {
        const completion = await client.chat.completions.create({
          model: "gpt-3.5-turbo",
          messages: [
            {
              role: "system",
              content: "You are an expert at analyzing software requirements and grouping them into meaningful categories. Given a list of related requirements, provide a concise, descriptive name (2-4 words) that captures the common theme or functionality. Examples: 'User Authentication', 'Payment Processing', 'Data Management', 'UI Components', 'Security Features'."
            },
            {
              role: "user",
              content: `Analyze these requirements and suggest a concise name for this cluster:\n\n${clusterRequirements.map((req, i) => `${i + 1}. ${req}`).join('\n')}\n\nProvide only the cluster name, nothing else.`
            }
          ],
          max_tokens: 20,
          temperature: 0.3
        });
        
        const name = completion.choices[0].message.content.trim().replace(/['"]/g, '');
        console.log(`âœ… Cluster ${clusterId} named: "${name}"`);
        return { clusterId, name };
      } catch (error) {
        console.warn(`âš ï¸ Failed to generate name for cluster ${clusterId}:`, error.message);
        return { clusterId, name: `Cluster ${clusterId}` };
      }
    })
  );

  // Create a name lookup map
  const nameMap = new Map();
  clusterNames.forEach(({ clusterId, name }) => {
    nameMap.set(clusterId, name);
  });

  clusters.forEach((indices, clusterId) => {
    const clusterPoints = indices.map((i) => reduced[i]);
    const cx = clusterPoints.reduce((a, b) => a + b[0], 0) / clusterPoints.length;
    const cy = clusterPoints.reduce((a, b) => a + b[1], 0) / clusterPoints.length;
    const radius = Math.max(
      ...clusterPoints.map(([x, y]) => Math.hypot(x - cx, y - cy))
    );
    clusterMap.set(clusterId, { 
      cx, 
      cy, 
      radius, 
      name: nameMap.get(clusterId) || `Cluster ${clusterId}` 
    });
  });

  // Combine reduced points with their cluster assignments
  const labeledPoints = reduced.map(([x, y], i) => {
    let clusterId = -1;
    clusters.forEach((indices, id) => {
      if (indices.includes(i)) clusterId = id;
    });
    return { text: requirements[i], x, y, cluster: clusterId };
  });

  return {
    points: labeledPoints,
    clusters: Array.from(clusterMap.entries()).map(([id, { cx, cy, radius, name }]) => ({
      id,
      cx,
      cy,
      radius,
      name,
    })),
  };
}

// Helper: compute adaptive k (number of clusters) from 2D point density
function computeAdaptiveK(points, n, opts = {}) {
  const { minK = 2, maxRatio = 0.5, scale = 2 } = opts;
  if (!Array.isArray(points) || points.length === 0) return Math.max(minK, 2);
  const xs = points.map(p => p[0]);
  const ys = points.map(p => p[1]);
  const width = Math.max(...xs) - Math.min(...xs);
  const height = Math.max(...ys) - Math.min(...ys);
  const area = Math.max(width * height, 1e-6); // avoid div by zero
  const density = n / area;
  // heuristic: more points and higher density -> more clusters
  const kRaw = Math.round(Math.sqrt(n * density) * scale);
  const maxK = Math.max(2, Math.floor(n * maxRatio));
  const k = Math.min(Math.max(minK, kRaw || minK), Math.max(2, Math.floor(n / 2), maxK));
  return k;
}

app.post("/embed", async (req, res) => {
  const { requirements } = req.body;
  console.log("ğŸ“© Received requirements:", requirements);

  if (!requirements || requirements.length === 0) {
    console.warn("âš ï¸ No requirements provided!");
    return res.status(400).json({ error: "No requirements provided" });
  }

  try {
    console.log("ğŸ§  Requesting embeddings from OpenAI...");
    const response = await client.embeddings.create({
      model: "text-embedding-3-small",
      input: requirements,
    });
    console.log("âœ… Embeddings received!");
    console.log("ğŸ” Embeddings:", response.data);

    const embeddings = response.data.map((item) => item.embedding);

    // --- Store in Marqo ---
    console.log("ğŸ’¾ Storing embeddings in Marqo...");

    try {
    // Create or reuse index
    await axios.post(`${MARQO_URL}/indexes/${INDEX_NAME}`, {
  model: "generic-embedding",
  model_properties: {
    "dimensions": 1536,
    "distance_metric": "cosine"
      }
    }).catch((err) => {
      if (err.response?.status !== 409) throw err;
    });

    // Prepare documents
    const docs = requirements.map((text, i) => ({
      _id: `req_${Date.now()}_${i}`,
      text,
      _embedding: embeddings[i],
    }));

    // Upload
    const res = await axios.post(
      `${MARQO_URL}/indexes/${INDEX_NAME}/documents`,
      { documents: docs }
    );

    console.log(`âœ… Stored ${docs.length} documents in Marqo`);
    return res.data;
  } catch (err) {
    console.error("âŒ Failed to store in Marqo:", err.response?.data || err.message);
    throw err;
  }

    // Step 1 â€” PCA reduction (1536D â†’ 2D)
    const pca = new PCA(embeddings);
    const reduced = pca.predict(embeddings, { nComponents: 2 }).to2DArray();

    // Step 2 â€” Run DBSCAN on the reduced 2D points
    const dbscan = new DBSCAN();
    
    // Adaptive clustering parameters based on number of requirements
    const numRequirements = requirements.length;
    let eps, minPts;
    
    if (numRequirements <= 5) {
      eps = 0.2;
      minPts = 2;
    } else if (numRequirements <= 15) {
      eps = 0.2;
      minPts = 3;
    } else if (numRequirements <= 30) {
      eps = 0.15;
      minPts = 5;
    } else {
      // For large datasets, skip DBSCAN and go straight to K-means
      console.log("ğŸ”„ Large dataset detected, using K-means directly...");
      console.log(`ğŸ“Š Debug: numRequirements = ${numRequirements}`);
      console.log(`ğŸ“Š Debug: reduced array length = ${reduced.length}`);
      console.log(`ğŸ“Š Debug: reduced[0] =`, reduced[0]);
      
      // compute k from point density (adaptive)
      const k = computeAdaptiveK(reduced, numRequirements, { minK: 2, maxRatio: 0.5, scale: 2 });
      console.log(`ğŸ”§ Using K-means with k=${k} clusters for ${numRequirements} points`);
      console.log(`ğŸ“Š Debug: k value = ${k} (type: ${typeof k})`);
      console.log(`ğŸ“Š Debug: reduced array type = ${Array.isArray(reduced) ? 'array' : typeof reduced}`);
      
      const kmeansResult = kmeans(reduced, k);
      console.log(`ğŸ“Š Debug: kmeansResult type =`, typeof kmeansResult);
      console.log(`ğŸ“Š Debug: kmeansResult =`, kmeansResult);
      
      // Convert K-means result to Map format
      const clusters = new Map();
      const clusterAssignments = kmeansResult.clusters; // K-means returns { clusters: [...] }
      for (let i = 0; i < k; i++) {
        const clusterIndices = [];
        clusterAssignments.forEach((clusterId, index) => {
          if (clusterId === i) {
            clusterIndices.push(index);
          }
        });
        if (clusterIndices.length > 0) {
          clusters.set(i, clusterIndices);
        }
      }
      
      console.log(`ğŸ“Š K-means results: ${clusters.size} clusters found`);
      clusters.forEach((indices, clusterId) => {
        console.log(`   Cluster ${clusterId}: ${indices.length} requirements`);
      });
      
      // Skip the rest of the DBSCAN logic and go directly to cluster naming
      const result = await processClusters(clusters, requirements, reduced, client);
      return res.json(result);
    }
    
    console.log(`ğŸ”§ Using DBSCAN parameters: eps=${eps}, minPts=${minPts} for ${numRequirements} requirements`);
    const dbscanResult = dbscan.run(reduced, eps, minPts);
    
    // Debug clustering results
    console.log(`ğŸ“Š DBSCAN result type:`, typeof dbscanResult);
    console.log(`ğŸ“Š DBSCAN result:`, dbscanResult);
    
    // Convert DBSCAN result to Map format
    const clusters = new Map();
    if (Array.isArray(dbscanResult)) {
      dbscanResult.forEach((indices, clusterId) => {
        if (indices && indices.length > 0) {
          clusters.set(clusterId, indices);
        }
      });
    }
    
    console.log(`ğŸ“Š Clustering results: ${clusters.size} clusters found`);
    clusters.forEach((indices, clusterId) => {
      console.log(`   Cluster ${clusterId}: ${indices.length} requirements`);
    });
    
    // Fallback: If DBSCAN produces only one cluster, use K-means instead
    if (clusters.size <= 1 && numRequirements > 5) {
      console.log("ğŸ”„ DBSCAN produced only one cluster, falling back to K-means...");
      console.log(`ğŸ“Š Debug: numRequirements = ${numRequirements}`);
      console.log(`ğŸ“Š Debug: reduced array length = ${reduced.length}`);
      
      const k = computeAdaptiveK(reduced, numRequirements, { minK: 2, maxRatio: 0.5, scale: 2 });
      console.log(`ğŸ”§ Using adaptive K-means with k=${k} clusters for ${numRequirements} points`);
      console.log(`ğŸ“Š Debug: k value = ${k} (type: ${typeof k})`);
      
      const kmeansResult = kmeans(reduced, k);
      
      // Convert K-means result to DBSCAN-like format
      clusters.clear();
      const clusterAssignments = kmeansResult.clusters; // K-means returns { clusters: [...] }
      for (let i = 0; i < k; i++) {
        const clusterIndices = [];
        clusterAssignments.forEach((clusterId, index) => {
          if (clusterId === i) {
            clusterIndices.push(index);
          }
        });
        if (clusterIndices.length > 0) {
          clusters.set(i, clusterIndices);
        }
      }
      
      console.log(`ğŸ“Š K-means results: ${clusters.size} clusters found`);
      clusters.forEach((indices, clusterId) => {
        console.log(`   Cluster ${clusterId}: ${indices.length} requirements`);
      });
    }

    // Process clusters and generate names
    const result = await processClusters(clusters, requirements, reduced, client);
    res.json(result);
  } catch (err) {
    console.error("âŒ Error during embedding:", err);
    res.status(500).json({ error: "Embedding failed", details: err.message });
  }
});

app.listen(port, () => {
  console.log(`ğŸš€ Server running at http://localhost:${port}`);
});